% !TeX root = ../report.tex
\section{Introduction}

With a steady increase in urbanization on top of already existing traffic problems in urban areas, the need for improving the traffic flow becomes ever more apparent. 
In urban areas, it is likely to be very costly if at all feasible to tear up existing infrastructure to accommodate better roads, so we turn our focus to the dynamic element present in most urban areas today, traffic lights.

Traffic lights provide an opportunity to influence the traffic flow in urban areas utilizing software, be it simple fixed-time, scheduled light changes or sophisticated traffic-reactive systems. 

In this project, we look into the online, adaptive optimization strategies through reinforcement learning. Some of the early adaptive approaches SCAT\cite{SCAT1980} which was implemented in Sydney around 1980 and SCOOT\cite{SCOOT1991}, which is in extensive use to this date, focus on the coordination of multiple intersections, often known as \emph{urban traffic control}. 
In \citet{SCOOT1991}, the coordination serves to shift traffic light cycles such that average queue lengths and vehicle stops are minimized, but still rely on the individual traffic light cycle to be efficient.

\vspace*{1em}

In this project, we seek to explore the application of reinforcement learning to control the individual traffic light, using only existing infrastructure.

At a very high level, reinforcement learning consist of an \textit{agent} placed into some \textit{envrionment}, where it takes actions and receive some \textit{reward} based on those actions. Over time, the goal of the agent is to learn which actions to take in what situations to maximize the expected future reward. We will only cover a small part of reinforcement learning in this project, for a wider overview refer to \citet{RLBook2018}.

The application of reinforcement learning to the field of traffic light control has been examined many a time\citep{Thorpe96trafficlight}\citep{wiering2000multi}\citep{AbdulhaiPringleKarakoulas}\citep{ItamarEtAl}\citep{bakker2010traffic}\citep{ExploringRewardDefinitions}\citep{ModelsAndAlgorithms}\citep{StateRepresentations}.
However, due to the data made available by swarco Danmark A/S, we can provide further insight into the details of applying reinforcement learning to traffic light control based on existing infrastructure and information.

\citet{Thorpe96trafficlight} showed great potential of reinforcement learning applied to traffic light control with their implementation of the SARSA algorithm on three different complete-knowledge state representations on a grid of 4x4 intersections, approaching the performance of an optimal policy.

\citet{wiering2000multi} presented three different state representations for the multi-agent problem, where varying degrees of global information was included in the states. 
He found that more global information did not necessarily mean better performance, but some global information did perform better than none.

\citet{AbdulhaiPringleKarakoulas} implemented Q-learning which is a temporal-difference algorithm, on a single-agent basis, with comments on ideas to improve it for multi-agent use. 
They used state information which could be estimated from detector data, in the form of queue lengths and elapsed phase time.  
The reward used is a penalty in the form of total vehicle delay between decision points. 
Additionally, they made use of a neural-network-like structure (CMAC) to approximate the value function, which provided some generalization and should, to some extent, improve behavior in unobserved states.

\citet{ItamarEtAl} used an actual neural network for value function approximation, which should perform even better in unobserved states compared to the CMAC. The state used is also significantly improved, as it incorporates only what they refer to as \emph{relative traffic flow}, calculated for every upstream lane as traffic flow in that lane divided by average traffic flow of all upstream lanes of the intersection. Such a state representation can be implemented using detector data only. The reward is based on average delay, giving a negative reward if an action increases the average delay, but a positive reward if it is decreased.

\citet{StateRepresentations} and \citet{ExploringRewardDefinitions} sought out to explore different state- and reward definitions respectively, which we will comment on later in the project, while \citet{ModelsAndAlgorithms} surveys the use of reinforcement learning in the field of urban traffic control, reflecting on existing research in the field.